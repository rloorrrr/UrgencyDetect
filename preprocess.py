# =========================================================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë° ê²½ë¡œ ì„¤ì •
# =========================================================
import os, json, glob, re, torch
import pandas as pd
import numpy as np
import librosa
from librosa.util import normalize
from tqdm.auto import tqdm
from transformers import AutoProcessor, AutoModel, AutoTokenizer
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler
from sklearn.metrics import f1_score, classification_report
from collections import Counter
import random
import copy


device = "cuda" if torch.cuda.is_available() else "cpu"

json_root = "/content/drive/MyDrive/urgency_extracted/TL_ê´‘ì£¼_êµ¬ì¡°"
wav_root  = "/content/drive/MyDrive/urgency_extracted/TS_ê´‘ì£¼_êµ¬ì¡°"


# =========================================================
# 2. JSON ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# =========================================================
json_files = glob.glob(os.path.join(json_root, "**/*.json"), recursive=True)
print(f"JSON íŒŒì¼ ìˆ˜: {len(json_files):,}")

data = []
for f in tqdm(json_files, desc="Loading JSON"):
    try:
        with open(f, "r", encoding="utf-8") as j:
            d = json.load(j)
            d["file_id"] = os.path.splitext(os.path.basename(f))[0]
            data.append(d)
    except Exception as e:
        print(f"âš ï¸ {f} ì½ê¸° ì‹¤íŒ¨: {e}")

df = pd.json_normalize(data)
print(f"ì´ {len(df):,}í–‰ ë¡œë“œ ì™„ë£Œ")

urgency_map = {"í•˜": 0, "ì¤‘": 1, "ìƒ": 2}
df["urgency_y"] = df["urgencyLevel"].astype(str).map(urgency_map)

def extract_text(u):
    try:
        if isinstance(u, str):
            u = json.loads(u)
        if isinstance(u, list):
            texts = [x.get("text", "") for x in u if isinstance(x, dict)]
            return " ".join(t for t in texts if t)
    except:
        return ""
    return ""

df["text"] = df["utterances"].apply(extract_text)
df = df[["file_id", "text", "urgency_y"]].dropna(subset=["urgency_y"])
df = df[df["text"].str.len() > 0].reset_index(drop=True)

# =========================================================
# 3. WAV íŒŒì¼ ë¡œë“œ ë° ë³‘í•© + ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
# =========================================================
wav_files = glob.glob(os.path.join(wav_root, "**/*.wav"), recursive=True)
df_wav = pd.DataFrame({"audio_path": wav_files})
df_wav["file_id"] = df_wav["audio_path"].apply(lambda x: os.path.splitext(os.path.basename(x))[0])

print(f"ğŸ§ WAV íŒŒì¼ ìˆ˜: {len(df_wav):,}")

# ì˜¤ë””ì˜¤ ê¸°ë³¸ í”¼ì²˜ ì¶”ì¶œ í•¨ìˆ˜
def extract_audio_features(path):
    try:
        y, sr = librosa.load(path, sr=16000)
        duration = librosa.get_duration(y=y, sr=sr)

        # ë¬´ì„±êµ¬ê°„ ì œì™¸í•œ ë°œí™” ë¹„ìœ¨
        non_silent = librosa.effects.split(y, top_db=25)
        voiced_time = sum((end - start) for start, end in non_silent) / sr
        speech_ratio = voiced_time / duration if duration > 0 else 0

        # í‰ê·  ì—ë„ˆì§€ (RMSÂ²)
        energy_mean = np.mean(y ** 2)

        # í”¼í¬ ì§„í­
        peak_amp = np.max(np.abs(y))

        # RMS (ë£¨íŠ¸ í‰ê·  ì œê³±)
        rms = np.sqrt(energy_mean)
        peak_to_rms = peak_amp / rms if rms > 0 else 0

        return pd.Series({
            "duration_sec": duration,
            "speech_ratio": speech_ratio,
            "energy_mean": energy_mean,
            "peak_amp": peak_amp,
            "peak_to_rms": peak_to_rms
        })
    except Exception as e:
        print(f"âš ï¸ {path} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.Series({
            "duration_sec": np.nan,
            "speech_ratio": np.nan,
            "energy_mean": np.nan,
            "peak_amp": np.nan,
            "peak_to_rms": np.nan
        })

# ì „ì²´ wav íŒŒì¼ì— ì ìš©
df_features = df_wav["audio_path"].apply(extract_audio_features)
df_wav = pd.concat([df_wav, df_features], axis=1)

print("âœ… ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ")
print(df_wav[["file_id", "duration_sec", "speech_ratio", "energy_mean", "peak_amp", "peak_to_rms"]].head())

#  JSONê³¼ ë³‘í•©
df_all = pd.merge(df, df_wav, on="file_id", how="inner")
print(f"ë³‘í•© ì™„ë£Œ: {len(df_all):,}í–‰")

# =========================================================
# 4. ì˜¤ë””ì˜¤ ì„ë² ë”© (Data2Vec-Audio)
# =========================================================

AUDIO_MODEL = "facebook/data2vec-audio-base"

from transformers import AutoProcessor, AutoModel
processor = AutoProcessor.from_pretrained(AUDIO_MODEL)
audio_model = AutoModel.from_pretrained(AUDIO_MODEL).to(device).eval()

def extract_audio_emb(path):
    try:
        y, sr = librosa.load(path, sr=16000)
        y = normalize(y)
        inputs = processor(y, sampling_rate=sr, return_tensors="pt", padding=True)
        with torch.no_grad():
            out = audio_model(**{k: v.to(device) for k, v in inputs.items()})
        emb = out.last_hidden_state.mean(dim=1).cpu().numpy().flatten()
        return emb / (np.linalg.norm(emb) + 1e-12)
    except Exception as e:
        print(f"[Audio ERR] {path}: {e}")
        return None

audio_embs = [extract_audio_emb(p) for p in tqdm(df_all["audio_path"], desc="Audio Embedding")]
df_all["audio_emb"] = audio_embs

# =========================================================
# 5. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
# =========================================================


TEXT_MODEL = "jhgan/ko-sroberta-multitask"

# ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL)
text_model = AutoModel.from_pretrained(TEXT_MODEL).to(device).eval()

def extract_text_emb(text):
    try:
        if not text or len(text.strip()) == 0:
            return None

        inputs = tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=384,
            return_tensors="pt"
        )

        with torch.no_grad():
            out = text_model(**{k: v.to(device) for k, v in inputs.items()})

        # [CLS] í† í°ì˜ ì„ë² ë”© ì¶”ì¶œ
        emb = out.last_hidden_state[:, 0, :]

        # Layer Normalization
        emb = F.layer_norm(emb, emb.shape[-1:])

        # L2 ì •ê·œí™”
        emb = emb.cpu().numpy().flatten()
        emb = emb / (np.linalg.norm(emb) + 1e-12)

        return emb
    except Exception as e:
        print(f"âš ï¸ í…ìŠ¤íŠ¸ ì„ë² ë”© ì‹¤íŒ¨: {e}")
        return None

# ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ ê°œì„ 
print(f"\nğŸ“ {len(df_all)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
text_embs = []

for text in tqdm(df_all["text"], desc="Text Embedding"):
    emb = extract_text_emb(text)
    text_embs.append(emb)

# df_allì— ì—…ë°ì´íŠ¸
df_all["text_emb"] = text_embs

# ì˜¤ë””ì˜¤ ì„ë² ë”© ë° í”¼ì²˜ ê²°í•©
audio_embs = np.vstack(df_all["audio_emb"].to_numpy())
audio_feat_cols = ["duration_sec", "speech_ratio", "energy_mean", "peak_amp", "peak_to_rms"]

if all(col in df_all.columns for col in audio_feat_cols):
    audio_feats = df_all[audio_feat_cols].fillna(0).to_numpy()
    Xa = np.hstack([audio_embs, audio_feats])
else:
    Xa = audio_embs

# í…ìŠ¤íŠ¸ ì„ë² ë”© ë° ë ˆì´ë¸”
Xt = np.vstack(df_all["text_emb"].to_numpy())
y = df_all["urgency_y"].to_numpy()

# ë“œë¼ì´ë¸Œ ì €ì¥ ê²½ë¡œ
save_dir = "/content/drive/MyDrive/embeddings_backup"
os.makedirs(save_dir, exist_ok=True)

# numpy ë°°ì—´ë¡œ ë³€í™˜ (None ê°’ í•„í„°ë§)
valid_df = df_all[df_all["audio_emb"].apply(lambda x: isinstance(x, np.ndarray)) &
                  df_all["text_emb"].apply(lambda x: isinstance(x, np.ndarray))].reset_index(drop=True)

audio_embs = np.vstack(valid_df["audio_emb"].to_numpy())
text_embs  = np.vstack(valid_df["text_emb"].to_numpy())

# ë©”íƒ€ë°ì´í„° ì €ì¥
meta_path = os.path.join(save_dir, "metadata.csv")
valid_df.drop(columns=["audio_emb", "text_emb"]).to_csv(meta_path, index=False)

# ì„ë² ë”© ì••ì¶• ì €ì¥
emb_path = os.path.join(save_dir, "embeddings.npz")
np.savez_compressed(emb_path, audio=audio_embs, text=text_embs)

print(f"âœ… ì €ì¥ ì™„ë£Œ!")
print(f"ğŸ“ ë©”íƒ€ë°ì´í„°: {meta_path}")
print(f"ğŸ“¦ ì„ë² ë”©: {emb_path}")
