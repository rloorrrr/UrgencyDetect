# =========================================================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
# =========================================================
import json, torch, librosa, numpy as np, os
from librosa.util import normalize
from transformers import AutoProcessor, AutoModel, AutoTokenizer
import torch.nn as nn
import torch.nn.functional as F

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âœ… Device: {device}")

# =========================================================
# 2. ì˜¤ë””ì˜¤ / í…ìŠ¤íŠ¸ ì„ë² ë”© ë„êµ¬ ë¡œë“œ
# =========================================================
AUDIO_MODEL = "facebook/data2vec-audio-base"
TEXT_MODEL  = "jhgan/ko-sroberta-multitask"

print("ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
audio_processor = AutoProcessor.from_pretrained(AUDIO_MODEL)
audio_model = AutoModel.from_pretrained(AUDIO_MODEL).to(device).eval()

tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL)
text_model = AutoModel.from_pretrained(TEXT_MODEL).to(device).eval()
print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")

# =========================================================
# 3. ëª¨ë¸ ì •ì˜ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
# =========================================================
class CrossAttentionFusionOrdinal(nn.Module):
    def __init__(self, audio_dim, text_dim=768, hidden_dim=768, num_classes=3, nhead=8):
        super().__init__()
        self.audio_proj = nn.Sequential(
            nn.Linear(audio_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.4)
        )
        self.text_proj = nn.Sequential(
            nn.Linear(text_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.4)
        )
        enc_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=nhead,
            dim_feedforward=2048,
            dropout=0.2,
            activation='gelu',
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=3)
        fusion_dim = hidden_dim * 4
        self.classifier = nn.Sequential(
            nn.Linear(fusion_dim, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.4),
            nn.Linear(512, num_classes)
        )

    def forward(self, a, t):
        a = self.audio_proj(a)
        t = self.text_proj(t)
        x = torch.stack([a, t], dim=1)
        h = self.encoder(x)
        pooled = torch.cat([
            h[:,0,:],
            h[:,1,:],
            torch.abs(h[:,0,:]-h[:,1,:]),
            h[:,0,:]*h[:,1,:]
        ], dim=-1)
        return self.classifier(pooled)

# =========================================================
# 4. ëª¨ë¸ ë¡œë“œ (ìë™ ì°¨ì› ê°ì§€)
# =========================================================
SAVE_PATH = "/content/drive/MyDrive/urgency_checkpoints_final/best_model_full.pt"

print(f"ğŸ“¦ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘: {SAVE_PATH}")

if not os.path.exists(SAVE_PATH):
    raise FileNotFoundError(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SAVE_PATH}")

# ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•´ì„œ audio_dim í™•ì¸
loaded = torch.load(SAVE_PATH, map_location=device, weights_only=False)

# audio_dim ìë™ ê°ì§€
audio_dim = None

if isinstance(loaded, nn.Module):
    # ì „ì²´ ëª¨ë¸ì´ ì €ì¥ëœ ê²½ìš°
    print("ğŸ” ì „ì²´ ëª¨ë¸ ê°ì§€")
    try:
        # audio_projì˜ ì²« ë²ˆì§¸ Linear ë ˆì´ì–´ì—ì„œ ì…ë ¥ ì°¨ì› ì¶”ì¶œ
        first_linear = loaded.audio_proj[0]
        audio_dim = first_linear.in_features
        print(f"âœ… ê°ì§€ëœ audio_dim: {audio_dim}")
    except:
        print("âš ï¸ audio_dim ìë™ ê°ì§€ ì‹¤íŒ¨")
    model = loaded
else:
    # state_dictê°€ ì €ì¥ëœ ê²½ìš°
    print("ğŸ” state_dict ê°ì§€")
    state_dict = loaded.get("model_state_dict", loaded)

    # audio_proj.0.weightì˜ shapeì—ì„œ ì…ë ¥ ì°¨ì› ì¶”ì¶œ
    first_layer_key = "audio_proj.0.weight"
    if first_layer_key in state_dict:
        audio_dim = state_dict[first_layer_key].shape[1]
        print(f"âœ… ê°ì§€ëœ audio_dim: {audio_dim}")
    else:
        print("âš ï¸ audio_dim ìë™ ê°ì§€ ì‹¤íŒ¨, ê¸°ë³¸ê°’ 768 ì‚¬ìš©")
        audio_dim = 768

    # ê°ì§€ëœ ì°¨ì›ìœ¼ë¡œ ëª¨ë¸ ìƒì„±
    model = CrossAttentionFusionOrdinal(audio_dim=audio_dim).to(device)
    model.load_state_dict(state_dict)
    print("âœ… state_dict ë¡œë“œ ì™„ë£Œ")

model.eval()
print(f"âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ (audio_dim={audio_dim})\n")

# ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥ (ì„ë² ë”© í•¨ìˆ˜ì—ì„œ ì‚¬ìš©)
MODEL_AUDIO_DIM = audio_dim

# =========================================================
# 5. í…ìŠ¤íŠ¸ ë° ì˜¤ë””ì˜¤ ì„ë² ë”© í•¨ìˆ˜
# =========================================================
def extract_text_from_json(json_path):

    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        utterances = data.get("utterances", [])
        texts = [u.get("text", "").strip() for u in utterances if isinstance(u, dict)]
        return " ".join([t for t in texts if t])
    except Exception as e:
        print(f"âŒ JSON ë¡œë“œ ì‹¤íŒ¨: {json_path}")
        print(f"   ì—ëŸ¬: {e}")
        return ""

def get_audio_emb(path, include_features=True):

    try:
        # 1. Data2Vec ì„ë² ë”© (768ì°¨ì›)
        y, sr = librosa.load(path, sr=16000)
        y = normalize(y)
        inputs = audio_processor(y, sampling_rate=sr, return_tensors="pt", padding=True)
        with torch.no_grad():
            out = audio_model(**{k: v.to(device) for k, v in inputs.items()})
        emb = out.last_hidden_state.mean(dim=1).cpu().numpy().flatten()

        if include_features:
            # 2. ì˜¤ë””ì˜¤ í”¼ì²˜ 5ê°œ ì¶”ì¶œ
            duration = librosa.get_duration(y=y, sr=sr)

            # ë¬´ì„±êµ¬ê°„ ì œì™¸í•œ ë°œí™” ë¹„ìœ¨
            non_silent = librosa.effects.split(y, top_db=25)
            voiced_time = sum((end - start) for start, end in non_silent) / sr
            speech_ratio = voiced_time / duration if duration > 0 else 0

            # í‰ê·  ì—ë„ˆì§€
            energy_mean = np.mean(y ** 2)

            # í”¼í¬ ì§„í­
            peak_amp = np.max(np.abs(y))

            # RMS ë° Peak-to-RMS
            rms = np.sqrt(energy_mean)
            peak_to_rms = peak_amp / rms if rms > 0 else 0

            # í”¼ì²˜ ë°°ì—´ ìƒì„±
            feats = np.array([duration, speech_ratio, energy_mean, peak_amp, peak_to_rms])

            # 3. ì„ë² ë”© + í”¼ì²˜ ê²°í•© (773ì°¨ì›)
            full_emb = np.concatenate([emb, feats])
        else:
            # í”¼ì²˜ ì—†ì´ 768ì°¨ì›ë§Œ
            full_emb = emb

        # 4. L2 ì •ê·œí™”
        return full_emb / (np.linalg.norm(full_emb) + 1e-12)

    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {path}")
        print(f"   ì—ëŸ¬: {e}")
        raise

def get_text_emb(text):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ (768ì°¨ì›)"""
    try:
        inputs = tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=384,
            return_tensors="pt"
        )

        with torch.no_grad():
            out = text_model(**{k: v.to(device) for k, v in inputs.items()})

        # [CLS] í† í°ì˜ ì„ë² ë”© ì¶”ì¶œ
        emb = out.last_hidden_state[:, 0, :]

        # Layer Normalization
        emb = F.layer_norm(emb, emb.shape[-1:])

        # L2 ì •ê·œí™”
        emb = emb.cpu().numpy().flatten()
        return emb / (np.linalg.norm(emb) + 1e-12)

    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨")
        print(f"   ì—ëŸ¬: {e}")
        raise

# =========================================================
# 6. ì¶”ë¡  í•¨ìˆ˜
# =========================================================
label_map = {0: "í•˜", 1: "ì¤‘", 2: "ìƒ"}

def predict_urgency_from_pair(json_path, wav_path, verbose=True):

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    if not os.path.exists(wav_path):
        raise FileNotFoundError(f"WAV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {wav_path}")

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = extract_text_from_json(json_path)
    if not text.strip():
        raise ValueError(f"âš ï¸ {json_path}ì— ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì°¨ì›ì— ë§ì¶° ì„ë² ë”© ì¶”ì¶œ
    include_features = (MODEL_AUDIO_DIM == 773)

    if verbose:
        if include_features:
            print("ğŸµ ì˜¤ë””ì˜¤ ì„ë² ë”© ì¶”ì¶œ ì¤‘ (773ì°¨ì›: ì„ë² ë”© + í”¼ì²˜)...")
        else:
            print("ğŸµ ì˜¤ë””ì˜¤ ì„ë² ë”© ì¶”ì¶œ ì¤‘ (768ì°¨ì›: ì„ë² ë”©ë§Œ)...")

    audio_emb = get_audio_emb(wav_path, include_features=include_features)

    if verbose:
        print("ğŸ“ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
    text_emb = get_text_emb(text)

    # í…ì„œ ë³€í™˜
    a = torch.FloatTensor(audio_emb).unsqueeze(0).to(device)
    t = torch.FloatTensor(text_emb).unsqueeze(0).to(device)

    if verbose:
        print(f"ğŸ” ì˜¤ë””ì˜¤ ì„ë² ë”© shape: {a.shape}")
        print(f"ğŸ” í…ìŠ¤íŠ¸ ì„ë² ë”© shape: {t.shape}")

    # ì¶”ë¡ 
    with torch.no_grad():
        logits = model(a, t)
        pred = logits.argmax(1).item()
        probs = torch.softmax(logits, dim=-1).cpu().numpy().flatten()

    return label_map[pred], probs, text

# =========================================================
# 7. ì˜ˆì‹œ ì‹¤í–‰
# =========================================================
print("=" * 60)
print("ğŸš€ ì¶”ë¡  ì‹œì‘")
print("=" * 60)

json_path = "YOUR_JSON"
wav_path  = "YOUR_WAV"

try:
    label, probs, text = predict_urgency_from_pair(json_path, wav_path, verbose=True)

    print("\n" + "=" * 60)
    print("ğŸ“Š ì¶”ë¡  ê²°ê³¼")
    print("=" * 60)
    print(f"ğŸ—£ í…ìŠ¤íŠ¸: {text[:200]}{'...' if len(text) > 200 else ''}")
    print(f"\nğŸ¯ ì˜ˆì¸¡ ê²°ê³¼: {label}")
    print(f"ğŸ“Š í´ë˜ìŠ¤ í™•ë¥ :")
    print(f"   - í•˜: {probs[0]:.4f} ({probs[0]*100:.2f}%)")
    print(f"   - ì¤‘: {probs[1]:.4f} ({probs[1]*100:.2f}%)")
    print(f"   - ìƒ: {probs[2]:.4f} ({probs[2]*100:.2f}%)")
    print("=" * 60)

except Exception as e:
    print("\n" + "=" * 60)
    print(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
    print("=" * 60)
    import traceback
    traceback.print_exc()
