# infer_emergency.py
# -*- coding: utf-8 -*-

import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import numpy as np
from transformers import AutoTokenizer, AutoModel, Wav2Vec2Processor, Wav2Vec2Model

# ==============================
# 1ï¸âƒ£ ì„¤ì •
# ==============================
SAVE_PATH = "./checkpoints/YOUR_PT"
TARGET_SAMPLE_RATE = 16000
MAX_TEXT_LENGTH = 256
MAX_AUDIO_LENGTH = TARGET_SAMPLE_RATE * 25  # 25ì´ˆ ì œí•œ
HIDDEN_DIM = 512
DROPOUT = 0.3

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… Using device: {device}")

# âœ… ì˜¤ë””ì˜¤ & JSON ê²½ë¡œ ì§€ì •
CUSTOM_AUDIO_PATH = "YOUR_WAV"
CUSTOM_JSON_PATH = "YOUR_JSON"

# ==============================
# 2ï¸âƒ£ ëª¨ë¸ ì •ì˜ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
# ==============================
class EmergencyClassifier(nn.Module):
    def __init__(self, text_encoder, audio_encoder, hidden_dim=HIDDEN_DIM, dropout=DROPOUT, num_classes=3):
        super().__init__()
        self.text_encoder = text_encoder
        self.audio_encoder = audio_encoder

        # íŒŒë¼ë¯¸í„° ë™ê²° ì •ì±…ì€ ì¸í¼ëŸ°ìŠ¤ì—ì„œëŠ” ë¶ˆí•„ìš”í•˜ì§€ë§Œ ìœ ì§€
        for param in self.text_encoder.parameters():
            param.requires_grad = False
        for param in self.audio_encoder.parameters():
            param.requires_grad = False

        text_dim = text_encoder.config.hidden_size  # 768
        audio_dim = audio_encoder.config.hidden_size  # 1024

        self.text_proj = nn.Sequential(
            nn.Linear(text_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        self.audio_proj = nn.Sequential(
            nn.Linear(audio_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, dropout=dropout, batch_first=True)

        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, input_ids, attention_mask, audio_input):
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_output = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
        text_emb = text_output.last_hidden_state[:, 0, :]  # [CLS]

        # ì˜¤ë””ì˜¤ ì¸ì½”ë”©
        audio_output = self.audio_encoder(audio_input)
        audio_emb = torch.mean(audio_output.last_hidden_state, dim=1)

        # ìž„ë² ë”© íˆ¬ì˜
        text_feat = self.text_proj(text_emb)
        audio_feat = self.audio_proj(audio_emb)

        combined = torch.cat([text_feat, audio_feat], dim=1)
        logits = self.classifier(combined)
        return logits


# ==============================
# 3ï¸âƒ£ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ==============================
print("ðŸ”¹ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")

koelectra_model_name = "monologg/koelectra-base-v3-discriminator"
wav2vec2_model_name = "kresnik/wav2vec2-large-xlsr-korean"

tokenizer = AutoTokenizer.from_pretrained(koelectra_model_name)
audio_processor = Wav2Vec2Processor.from_pretrained(wav2vec2_model_name)
text_encoder = AutoModel.from_pretrained(koelectra_model_name)
audio_encoder = Wav2Vec2Model.from_pretrained(wav2vec2_model_name)

model = EmergencyClassifier(text_encoder, audio_encoder).to(device)

checkpoint = torch.load(SAVE_PATH, map_location=device)
model.load_state_dict(checkpoint["model_state_dict"])
model.eval()

print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")


# ==============================
# 4ï¸âƒ£ ì˜ˆì¸¡ í•¨ìˆ˜
# ==============================
def predict_from_json_and_audio(json_path, audio_path):
    # JSON ë¡œë“œ
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    utterances = [u["text"] for u in data["utterances"] if "text" in u]
    full_text = " ".join(utterances).strip()
    if not full_text:
        full_text = " "

    # ì˜¤ë””ì˜¤ ë¡œë“œ
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"Audio file not found: {audio_path}")

    waveform, sr = torchaudio.load(audio_path)

    # ë¦¬ìƒ˜í”Œë§
    if sr != TARGET_SAMPLE_RATE:
        waveform = torchaudio.transforms.Resample(sr, TARGET_SAMPLE_RATE)(waveform)

    # ëª¨ë…¸ ë³€í™˜
    if waveform.shape[0] > 1:
        waveform = torch.mean(waveform, dim=0, keepdim=True)
    waveform = waveform.squeeze(0)

    # ê¸¸ì´ ì¡°ì •
    if waveform.shape[0] > MAX_AUDIO_LENGTH:
        waveform = waveform[:MAX_AUDIO_LENGTH]
    elif waveform.shape[0] < TARGET_SAMPLE_RATE:
        pad = TARGET_SAMPLE_RATE - waveform.shape[0]
        waveform = F.pad(waveform, (0, pad))

    # í† í°í™”
    text_inputs = tokenizer(
        full_text,
        max_length=MAX_TEXT_LENGTH,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )

    audio_inputs = audio_processor(
        waveform.numpy(),
        sampling_rate=TARGET_SAMPLE_RATE,
        return_tensors="pt",
        padding=True,
        max_length=MAX_AUDIO_LENGTH,
        truncation=True
    )

    with torch.no_grad():
        input_ids = text_inputs["input_ids"].to(device)
        attention_mask = text_inputs["attention_mask"].to(device)
        audio_input = audio_inputs["input_values"].to(device)

        logits = model(input_ids, attention_mask, audio_input)
        probs = torch.softmax(logits, dim=1)
        pred_label = torch.argmax(probs, dim=1).item()

    label_map = {0: "í•˜", 1: "ì¤‘", 2: "ìƒ"}

    print(f"\nðŸ—£ï¸ í…ìŠ¤íŠ¸ ì˜ˆì‹œ: {full_text[:100]}...")
    print(f"ðŸŽ§ ì˜¤ë””ì˜¤ íŒŒì¼: {audio_path}")
    print(f"âœ… ì˜ˆì¸¡ëœ ê¸´ê¸‰ë„: {label_map[pred_label]}")
    print(f"ðŸ“Š í™•ë¥ ë¶„í¬: {probs.squeeze().cpu().numpy()}")

    return label_map[pred_label]


# ==============================
# 5ï¸âƒ£ ì‹¤í–‰
# ==============================
if __name__ == "__main__":
    result = predict_from_json_and_audio(CUSTOM_JSON_PATH, CUSTOM_AUDIO_PATH)
    print(f"\nìµœì¢… ì˜ˆì¸¡ ê²°ê³¼: {result}")
