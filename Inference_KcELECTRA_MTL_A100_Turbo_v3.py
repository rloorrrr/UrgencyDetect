# -*- coding: utf-8 -*-
"""Inference_KcELECTRA_MTL_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b4PzgilX-f_u2Dj9zLJ2lqvbg_Yr5kFM

# ğŸš€ KcELECTRA MTL (v3) â€” ì¶”ë¡  ë° ì„±ëŠ¥ ê²€ì¦

í•™ìŠµëœ `best_mtl.pt` ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ **í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ì„ ê²€ì¦**í•˜ê³ , **ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì¶”ë¡ **ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**ì‹¤í–‰ ë°©ë²•:**
1.  **[í•„ìˆ˜]** **1ë²ˆ ì…€**ì„ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë“œë¼ì´ë¸Œë¥¼ ë§ˆìš´íŠ¸í•˜ê³  í•™ìŠµ í™˜ê²½ê³¼ ë™ì¼í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
2.  1ë²ˆ ì…€ ì‹¤í–‰ ì™„ë£Œ ì‹œ **ëŸ°íƒ€ì„ì´ ìë™ìœ¼ë¡œ ì¬ì‹œì‘**ë©ë‹ˆë‹¤.
3.  ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„, **2ë²ˆ ì…€ë¶€í„°** ë§ˆì§€ë§‰ ì…€ê¹Œì§€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

## 1. ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ ë° í™˜ê²½ êµ¬ì„±

í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸(`kcelectra_mtl_a100_turbo_v3.py`)ì™€ ë™ì¼í•œ í™˜ê²½ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
"""

# â¬‡ï¸ Google Drive & deps
from google.colab import drive
drive.mount('/content/drive')

# === Clean & Pin: Colab ê¸°ë³¸ ìŠ¤íƒê³¼ í˜¸í™˜ë˜ëŠ” ì¡°í•©ìœ¼ë¡œ ë§ì¶¥ë‹ˆë‹¤. ===
import sys, subprocess, os

def pip(*args):
    print(">>", " ".join(args))
    subprocess.check_call([sys.executable, "-m", "pip", *args])

# 1) ì¶©ëŒë‚˜ëŠ” í•µì‹¬ íŒ¨í‚¤ì§€ ì œê±°
pip("uninstall", "-y", "pyarrow", "pandas", "numpy", "fastparquet", "scikit-learn")

# 2) í˜¸í™˜ ì¡°í•©ìœ¼ë¡œ ì„¤ì¹˜
pip("install", "-q", "numpy==2.0.2", "pandas==2.2.2", "pyarrow==16.1.0", "scikit-learn==1.5.2")

# 3) NLP ìŠ¤íƒ(Transformers/Accelerate)ì€ 'ì˜ì¡´ì„± ìë™ ì—…ê·¸ë ˆì´ë“œ'ë¥¼ ë§‰ìŠµë‹ˆë‹¤.
pip("install", "-q", "--no-deps", "transformers>=4.44.0", "accelerate>=0.33.0", "tqdm>=4.66.4", "pandas==2.2.2")

print("\nâœ… Reinstall done. Now restarting kernel to finalize...")
os.kill(os.getpid(), 9)  # ğŸ” ëŸ°íƒ€ì„ ê°•ì œ ì¬ì‹œì‘

"""--- (ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„ ì—¬ê¸°ë¶€í„° ì‹¤í–‰) ---

## 2. ê¸°ë³¸ ì„¤ì • ë° ì„í¬íŠ¸
"""

import torch, platform, warnings
import os, random, numpy as np, pandas as pd
from pathlib import Path
from tqdm.auto import tqdm
from torch.amp import autocast
import torch.nn as nn
from torch.utils.data import DataLoader
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from transformers import AutoTokenizer, AutoModel

warnings.filterwarnings('ignore')
print("PyTorch:", torch.__version__, "| CUDA:", torch.version.cuda)
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
print("Python:", platform.python_version())

# âš™ï¸ Config (í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •)
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ê²½ë¡œ ì„¤ì •
DATA_DIR  = Path("/content/drive/MyDrive/project1/Processed_Data/MTL_ready_v3")  # âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œìš©
SAVE_DIR  = Path("/content/drive/MyDrive/project1/kcelectra_mtl_ckpt_v3") # âœ… ëª¨ë¸ ë¡œë“œìš©
CACHE_DIR = Path("/content/kcelectra_mtl_cache") # âœ… í…ŒìŠ¤íŠ¸ ì²­í¬ ìºì‹œìš©
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# AMP ì„¤ì •
bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()
amp_dtype = torch.bfloat16 if bf16_ok else torch.float16
if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (ëª¨ë¸ êµ¬ì¡°/í† í¬ë‚˜ì´ì € ì •ì˜ì— í•„ìš”í•œ ë¶€ë¶„ë§Œ)
BATCH_SIZE = 160 # í‰ê°€ ì‹œ ë°°ì¹˜ í¬ê¸° (OOM ì‹œ 128, 96 ë“±ìœ¼ë¡œ ê°ì†Œ)
NUM_WORKERS = 4
PREFETCH = 4
PIN_MEMORY = True
PERSISTENT = True

MAX_LEN = 384
STRIDE = 192
DROPOUT_RATE = 0.3 # ëª¨ë¸ êµ¬ì¡° ì •ì˜ ì‹œ í•„ìš”
MODEL_NAME = "beomi/KcELECTRA-base-v2022"

print(f"\nğŸ“‹ Config:")
print(f"  AMP dtype: {amp_dtype}")
print(f"  MAX_LEN/STRIDE: {MAX_LEN}/{STRIDE}")
print(f"  Model Path: {SAVE_DIR}")
print(f"  Data Path: {DATA_DIR}")

"""## 3. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜

í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ **ì •í™•íˆ ë™ì¼í•œ** `ElectraMTL` í´ë˜ìŠ¤ë¥¼ ì •ì˜í•´ì•¼ `state_dict`ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

class ElectraMTL(nn.Module):
    def __init__(self, base_name: str, n_u: int = 3, n_s: int = 4, dropout=0.3):
        super().__init__()
        self.base = AutoModel.from_pretrained(base_name)
        hdim = self.base.config.hidden_size
        self.dropout = nn.Dropout(dropout)
        self.head_u = nn.Linear(hdim, n_u)
        self.head_s = nn.Linear(hdim, n_s)

    def forward(self, input_ids, attention_mask):
        out = self.base(input_ids=input_ids, attention_mask=attention_mask)
        cls = out.last_hidden_state[:, 0]
        z = self.dropout(cls)
        return self.head_u(z), self.head_s(z)

"""## 4. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ"""

print(f"\nğŸ”¤ Loading tokenizer: {MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

print("ğŸ—ï¸ Building model structure...")
model = ElectraMTL(MODEL_NAME, n_u=3, n_s=4, dropout=DROPOUT_RATE).to(device)
model.eval() # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •

ckpt_path = SAVE_DIR / "best_mtl.pt"
if ckpt_path.exists():
    try:
        print(f"ğŸ“¥ Loading weights from {ckpt_path}")
        state = torch.load(ckpt_path, map_location=device, weights_only=False)
        model.load_state_dict(state["state_dict"])
        print(f"âœ… Model loaded successfully from epoch {state.get('epoch', 'N/A')}")
        print(f"   Best combo score: {state.get('best_combo', 0.0):.4f}")

        # ì €ì¥ëœ config ë¡œë“œ (ì°¸ê³ ìš©)
        loaded_config = state.get("config", {})
        print(f"   Loaded Config (ML/ST): {loaded_config.get('max_len', 'N/A')}/{loaded_config.get('stride', 'N/A')}")
        # í˜„ì¬ ì„¤ì •ê³¼ ì €ì¥ëœ ì„¤ì •ì´ ë‹¤ë¥¼ ê²½ìš° ê²½ê³ 
        if loaded_config.get('max_len', MAX_LEN) != MAX_LEN or loaded_config.get('stride', STRIDE) != STRIDE:
            print("âš ï¸ WARNING: Current MAX_LEN/STRIDE differs from saved model config!")

    except Exception as e:
        print(f"âŒ Error loading model: {e}")
else:
    print(f"âŒ ERROR: Model checkpoint not found at {ckpt_path}")
    print("Please check the SAVE_DIR path or train the model first.")

"""## 5. í—¬í¼ í•¨ìˆ˜ ì •ì˜ (ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬)

ì„±ëŠ¥ ê²€ì¦ì„ ìœ„í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""

# 1. ë°ì´í„° ë¡œë“œ í—¬í¼
def read_any(base: Path, stem: str):
    """Parquet â†’ CSV â†’ Feather ìˆœìœ¼ë¡œ ì‹œë„"""
    for ext, fn in [(".parquet", pd.read_parquet),
                    (".csv", pd.read_csv),
                    (".feather", pd.read_feather)]:
        fp = base / f"{stem}{ext}"
        if fp.exists():
            print(f"  â†’ {fp.name}")
            try:
                return fn(fp)
            except Exception as e:
                print(f"âš ï¸ Failed to read {fp.name}: {e}. Try installing engines or checking versions.")
    raise FileNotFoundError(f"{stem} not found in {base}")

def squeeze_label(obj, key=None):
    if isinstance(obj, pd.DataFrame):
        if key and key in obj.columns:
            return obj[key].astype(int)
        if obj.shape[1] == 1:
            return obj.iloc[:, 0].astype(int)
    return pd.Series(obj).astype(int)

# 2. ì²­í¬ ìºì‹œ í—¬í¼
def build_chunk_cache_MTL(texts, y_u, y_s, tokenizer, split_name: str,
                          max_len=512, stride=256, batch_size=1024):
    """ì²­í¬ ë‹¨ìœ„ë¡œ í† í¬ë‚˜ì´ì§• + ìºì‹±"""
    cache_path = CACHE_DIR / f"{split_name}_ml{max_len}_st{stride}.pt"

    if cache_path.exists():
        try:
            print(f"âš¡ Loading cached chunks: {cache_path.name}")
            cached = torch.load(cache_path, weights_only=False)
            expected_keys = {"input_ids", "attention_mask", "y_u", "y_s", "sample_idx"}
            if expected_keys.issubset(cached.keys()):
                if cached["input_ids"].dtype == torch.int32 and cached["attention_mask"].dtype == torch.int8:
                    print(f"  Chunks: {cached['y_u'].size(0):,}")
                    return cached
                else:
                    print("  âš ï¸ Cache dtype/shape invalid, regenerating...")
            else:
                print("  âš ï¸ Invalid cache, regenerating...")
        except Exception as e:
            print(f"  âš ï¸ Cache load failed: {e}, regenerating...")

    print(f"ğŸ”¨ Building cache: {split_name} (MAX_LEN={max_len}, STRIDE={stride})")
    all_ids, all_masks, all_u, all_s, all_map = [], [], [], [], []
    N = len(texts)

    for i in tqdm(range(0, N, batch_size), desc=f"Tokenize [{split_name}]"):
        batch_texts = [str(t) for t in texts[i:i+batch_size]]

        enc = tokenizer(
            batch_texts,
            max_length=max_len,
            truncation=True,
            padding="max_length",
            return_attention_mask=True,
            return_overflowing_tokens=True,
            stride=stride,
            return_tensors="pt"
        )

        mapping = enc.pop("overflow_to_sample_mapping")
        global_map = (mapping + i).to(torch.int32)

        ids = enc["input_ids"].to(torch.int32)
        mask = enc["attention_mask"].to(torch.int8)

        labs_u = torch.tensor([int(y_u.iloc[int(j)]) for j in global_map],
                              dtype=torch.int64)
        labs_s = torch.tensor([int(y_s.iloc[int(j)]) for j in global_map],
                              dtype=torch.int64)

        all_ids.append(ids)
        all_masks.append(mask)
        all_u.append(labs_u)
        all_s.append(labs_s)
        all_map.append(global_map)

    tensors = {
        "input_ids": torch.cat(all_ids, 0),
        "attention_mask": torch.cat(all_masks, 0),
        "y_u": torch.cat(all_u, 0),
        "y_s": torch.cat(all_s, 0),
        "sample_idx": torch.cat(all_map, 0),
    }

    torch.save(tensors, cache_path)
    print(f"ğŸ’¾ Saved cache â†’ {cache_path.name} | Chunks: {tensors['y_u'].size(0):,}")
    return tensors

# 3. Dataset í´ë˜ìŠ¤
class MTLChunkDataset(torch.utils.data.Dataset):
    def __init__(self, tensors):
        self.t = tensors

    def __len__(self):
        return self.t["y_u"].size(0)

    def __getitem__(self, i):
        return {
            "input_ids": self.t["input_ids"][i].long(),
            "attention_mask": self.t["attention_mask"][i].bool(),
            "y_u": self.t["y_u"][i],
            "y_s": self.t["y_s"][i],
        }

# 4. DataLoader ìƒì„± í—¬í¼
def make_loader(ds, batch_size, shuffle):
    try:
        return DataLoader(
            ds, batch_size=batch_size, shuffle=shuffle,
            num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,
            prefetch_factor=PREFETCH, persistent_workers=PERSISTENT
        )
    except Exception as e:
        print(f"âš ï¸ DataLoader fallback (workers=0): {e}")
        return DataLoader(ds, batch_size=batch_size, shuffle=shuffle,
                         num_workers=0, pin_memory=False)

"""## 6. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬

ì„±ëŠ¥ ê²€ì¦ì„ ìœ„í•´ `DATA_DIR`ì—ì„œ í…ŒìŠ¤íŠ¸ì…‹(`X_test`, `y_test_...`)ì„ ë¡œë“œí•©ë‹ˆë‹¤.
"""

print("\nğŸ“‚ Loading TEST data...")
try:
    X_test  = read_any(DATA_DIR, "X_test")
    y_u_te  = read_any(DATA_DIR, "y_test_urgency")
    y_s_te  = read_any(DATA_DIR, "y_test_sentiment")

    y_u_te = squeeze_label(y_u_te, "y_urgency")
    y_s_te = squeeze_label(y_s_te, "y_sentiment")
    text_te = X_test["full_text"].astype(str)

    print(f"âœ… Test data loaded: {len(text_te):,}")

    print("\nğŸ”„ Preparing test chunks...")
    va_cache = build_chunk_cache_MTL(text_te, y_u_te, y_s_te, tokenizer, "val", MAX_LEN, STRIDE)

    print("\nğŸšš Creating Test DataLoader...")
    val_loader = make_loader(MTLChunkDataset(va_cache), BATCH_SIZE * 2, False)
    print(f"  Test batches: {len(val_loader):,}")

except FileNotFoundError as e:
    print(f"âŒ ERROR: Test data not found in {DATA_DIR}. {e}")
    print("Performance check cannot proceed. Skipping to inference demo.")
    val_loader = None
except Exception as e:
    print(f"âŒ ERROR loading data: {e}")
    val_loader = None

"""## 7. í‰ê°€ í•¨ìˆ˜ ì •ì˜

í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì˜ `evaluate_call_level` í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

@torch.no_grad()
def evaluate_call_level(model, loader, cache_tensors):
    """Call-level í‰ê°€ (ì²­í¬ ë¡œì§“ í‰ê· )"""
    if loader is None:
        print("Loader is not available. Skipping evaluation.")
        return None

    model.eval()
    logits_u_list, logits_s_list = [], []
    labels_u_list, labels_s_list = [], []

    for batch in tqdm(loader, desc="Eval", leave=False):
        ids = batch["input_ids"].to(device, non_blocking=True)
        mask = batch["attention_mask"].to(device, non_blocking=True)
        yu = batch["y_u"].to(device, non_blocking=True)
        ys = batch["y_s"].to(device, non_blocking=True)

        with autocast('cuda', dtype=amp_dtype, enabled=torch.cuda.is_available()):
            lu, ls = model(ids, mask)

        # FP32 ìºìŠ¤íŒ… (ë„˜íŒŒì´ ë³€í™˜ ì „)
        logits_u_list.append(lu.detach().float().cpu().numpy())
        logits_s_list.append(ls.detach().float().cpu().numpy())
        labels_u_list.append(yu.detach().cpu().numpy())
        labels_s_list.append(ys.detach().cpu().numpy())

    # ì—°ê²°
    all_logits_u = np.concatenate(logits_u_list, 0)
    all_logits_s = np.concatenate(logits_s_list, 0)
    all_labels_u = np.concatenate(labels_u_list, 0)
    all_labels_s = np.concatenate(labels_s_list, 0)

    idx = cache_tensors["sample_idx"].cpu().numpy()

    # Call-level ì§‘ê³„
    results = {}
    for task_name, logits, ytrue in [
        ("urgency", all_logits_u, all_labels_u),
        ("sentiment", all_logits_s, all_labels_s)
    ]:
        preds, gts = [], []
        for sid in np.unique(idx):
            mask = (idx == sid)
            pooled_logits = logits[mask].mean(0)
            preds.append(pooled_logits.argmax())
            gts.append(ytrue[mask][0])

        preds = np.array(preds)
        gts = np.array(gts)

        results[task_name] = {
            "micro": f1_score(gts, preds, average="micro"),
            "macro": f1_score(gts, preds, average="macro"),
            "weighted": f1_score(gts, preds, average="weighted"),
            "per_class": f1_score(gts, preds, average=None).tolist(),
            "predictions": preds,
            "ground_truth": gts,
            "report": classification_report(gts, preds, output_dict=True)
        }

    return results

"""## 8. ğŸ“Š í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ ê²€ì¦ ì‹¤í–‰

ë¡œë“œëœ `best_mtl.pt` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ì„ í™•ì¸í•©ë‹ˆë‹¤.
"""

print("\nğŸ“Š Final Evaluation on Test Set (using loaded model):")
final_results = evaluate_call_level(model, val_loader, va_cache)

if final_results:
    print(f"\n{'='*70}")
    print("         TEST SET PERFORMANCE REPORT")
    print(f"{'='*70}")

    print("\nUrgency:")
    print(f"  Weighted F1: {final_results['urgency']['weighted']:.4f}")
    print(f"  Per-class F1: {final_results['urgency']['per_class']}")
    print("  Classification Report (Dict):")
    print(final_results['urgency']['report'])

    print("\nSentiment:")
    print(f"  Weighted F1: {final_results['sentiment']['weighted']:.4f}")
    print(f"  Per-class F1: {final_results['sentiment']['per_class']}")
    print("  Classification Report (Dict):")
    print(final_results['sentiment']['report'])

    print("\nğŸ“ˆ Confusion Matrix (Urgency):")
    cm_u = confusion_matrix(final_results['urgency']['ground_truth'],
                            final_results['urgency']['predictions'])
    print(cm_u)

    print("\nğŸ“ˆ Confusion Matrix (Sentiment):")
    cm_s = confusion_matrix(final_results['sentiment']['ground_truth'],
                            final_results['sentiment']['predictions'])
    print(cm_s)

"""## 9. ì¶”ë¡  í•¨ìˆ˜ ì •ì˜ (ìƒˆ í…ìŠ¤íŠ¸ìš©)

í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì˜ `predict_text` í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

@torch.no_grad()
def predict_text(texts, aggregate="mean"):
    """ê¸´ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ (ì²­í¬ í‰ê· )"""
    if isinstance(texts, str):
        texts = [texts]

    model.eval()
    results = []

    for text in texts:
        enc = tokenizer(
            text,
            max_length=MAX_LEN,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
            return_overflowing_tokens=True,
            stride=STRIDE
        ).to(device)

        with autocast('cuda', dtype=amp_dtype, enabled=torch.cuda.is_available()):
            lu, ls = model(
                enc["input_ids"],
                enc["attention_mask"]
            )
            prob_u = lu.softmax(-1).float()  # FP32 ìºìŠ¤íŒ…
            prob_s = ls.softmax(-1).float()  # FP32 ìºìŠ¤íŒ…

        # ì²­í¬ í‰ê· 
        if aggregate == "mean":
            avg_u = prob_u.mean(0)
            avg_s = prob_s.mean(0)
        else:  # max (í˜¹ì€ ë‹¤ë¥¸ ì§‘ê³„ ë°©ì‹)
            avg_u = prob_u.max(0).values
            avg_s = prob_s.max(0).values

        pred_u = int(avg_u.argmax().item())
        pred_s = int(avg_s.argmax().item())
        conf_u = float(avg_u.max().item())
        conf_s = float(avg_s.max().item())

        results.append({
            "urgency": pred_u,
            "sentiment": pred_s,
            "urgency_conf": conf_u,
            "sentiment_conf": conf_s
        })

    return results[0] if len(results) == 1 else results

"""## 10. ğŸ§ª ì¶”ë¡  ë°ëª¨

ì„ì˜ì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í™•ì¸í•©ë‹ˆë‹¤.
"""

print("\nğŸ§ª Test Inference:")

# (ë¼ë²¨ ë§µ ì°¸ê³ : Urgency={0:'í•˜', 1:'ì¤‘', 2:'ìƒ'} / Sentiment={0:'anxious', 1:'angry', 2:'neutral', 3:'sad'})

text_1 = "[TURN=1] [CALLER] ì•„ ë„¤ ì•ˆë…•í•˜ì„¸ìš” [TURN=2] [AGENT] ë„¤ ì•ˆë…•í•˜ì„¸ìš” [TURN=3] [CALLER] ì €ê¸°ìš” ì œê°€ ì§€ê¸ˆ ì†ê°€ë½ì„ ì¢€ ë‹¤ì³¤ëŠ”ë° í”¼ê°€ ë„ˆë¬´ ë§ì´ ë‚˜ê°€ì§€ê³ ìš” [TURN=4] [AGENT] ë„¤ ì†ê°€ë½ ì–´ë”” ë‹¤ì¹˜ì…¨ì–´ìš” [TURN=5] [CALLER] ì§€ê¸ˆ ê·¸ ë² ì—¬ê°€ì§€ê³  í”¼ê°€ ë©ˆì¶”ì§ˆ ì•Šì•„ìš” ë„ˆë¬´ ì•„íŒŒìš”"
text_2 = "[TURN=1] [CALLER] ì €ê¸° ì°¨ê°€ ë°©ì „ì´ ë¼ì„œ ê·¸ëŸ¬ëŠ”ë°ìš” [TURN=2] [AGENT] ì•„ ë„¤ ì°¨ëŸ‰ ë°©ì „ì´ìš” ìœ„ì¹˜ê°€ ì–´ë””ì‹œì£  [TURN=3] [CALLER] ì—¬ê¸°ê°€ ê·¸ ê°•ë‚¨ì—­ 3ë²ˆ ì¶œêµ¬ ì•ì´ê±°ë“ ìš” [TURN=4] [AGENT] ë„¤ ì•Œê² ìŠµë‹ˆë‹¤ ê¸ˆë°© ê¸°ì‚¬ë‹˜ ë³´ë‚´ë“œë¦´ê²Œìš” [TURN=5] [CALLER] ë„¤ ê°ì‚¬í•©ë‹ˆë‹¤"

print("--- Sample 1 (ê¸´ê¸‰/ë¶ˆì•ˆ) ---")
result_1 = predict_text(text_1)
print(f"Input: {text_1[:100]}...")
print(f"Prediction: {result_1}")

print("--- Sample 2 (ë¹„ê¸´ê¸‰/í‰ì •) ---")
result_2 = predict_text(text_2)
print(f"Input: {text_2[:100]}...")
print(f"Prediction: {result_2}")

print("\nâœ… Inference Demo Done!")