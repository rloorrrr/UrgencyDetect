# -*- coding: utf-8 -*-
"""KcELECTRA_MTL_A100_Turbo_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XZRGRb7w8_OaWlzltY8NaJEnnXmXwK_f
"""

# â¬‡ï¸ Google Drive & deps
from google.colab import drive
drive.mount('/content/drive')

# === Clean & Pin: Colab ê¸°ë³¸ ìŠ¤íƒê³¼ í˜¸í™˜ë˜ëŠ” ì¡°í•©ìœ¼ë¡œ ë§ì¶¥ë‹ˆë‹¤. ===
import sys, subprocess, os

def pip(*args):
    print(">>", " ".join(args))
    subprocess.check_call([sys.executable, "-m", "pip", *args])

# 1) ì¶©ëŒë‚˜ëŠ” í•µì‹¬ íŒ¨í‚¤ì§€ ì œê±°
pip("uninstall", "-y", "pyarrow", "pandas", "numpy", "fastparquet", "scikit-learn")

# 2) í˜¸í™˜ ì¡°í•©ìœ¼ë¡œ ì„¤ì¹˜
# - Colab ê¸°ë³¸: pandas==2.2.2 (google-colabê°€ ì´ ë²„ì „ì„ ê³ ì • ì‚¬ìš©)
# - NumPy 2.0.2 ìœ ì§€ (Colab ê¸°ë³¸)
# - pyarrow 16.xëŠ” NumPy 2 ëŒ€ì‘ ì•ˆì •, pandas 2.2.2ì™€ë„ í˜¸í™˜ OK
# - scikit-learn 1.5.xë¡œ ìƒí–¥(ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ìš”êµ¬)
pip("install", "-q", "numpy==2.0.2", "pandas==2.2.2", "pyarrow==16.1.0", "scikit-learn==1.5.2")

# 3) NLP ìŠ¤íƒ(Transformers/Accelerate)ì€ 'ì˜ì¡´ì„± ìë™ ì—…ê·¸ë ˆì´ë“œ'ë¥¼ ë§‰ìŠµë‹ˆë‹¤.
#    (google-colab ê³ ì • íŒ¨í‚¤ì§€ë¥¼ ê±´ë“œë¦¬ì§€ ì•Šë„ë¡ --no-deps)
pip("install", "-q", "--no-deps", "transformers>=4.44.0", "accelerate>=0.33.0", "tqdm>=4.66.4", "pandas==2.2.2")

print("\nâœ… Reinstall done. Now restarting kernel to finalize...")
os.kill(os.getpid(), 9)  # ğŸ” ëŸ°íƒ€ì„ ê°•ì œ ì¬ì‹œì‘

# -*- coding: utf-8 -*-
"""KcELECTRA_MTL_A100_Turbo_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XZRGRb7w8_OaWlzltY8NaJEnnXmXwK_f

# ğŸš€ KcELECTRA MTL â€” A100 Turbo (v3, FP32-cast fix)

**Shared encoder + 2 heads** (Urgency 3-class, Sentiment 4-class)

- âœ… Call-level F1 (chunkâ†’call ë¡œì§“ í‰ê· )
- âœ… AMP ìµœì‹  ë¬¸ë²• (`torch.amp.autocast('cuda', ...)`)
- âœ… BF16/FP16 ì‚¬ìš© ì‹œ **ë„˜íŒŒì´ ë³€í™˜ ì „ FP32 ìºìŠ¤íŒ…**(ì´ë²ˆ ë²„ê·¸ í”½ìŠ¤)
- âœ… MAX_LEN/STRIDE ìŠ¤ìœ• ë„ìš°ë¯¸ (íŠœë‹íŒ)
- âœ… DataLoader íŠœë‹ / ë¡œì»¬ ìºì‹œ (í„°ë³´íŒ)

- âœ… /content ë¡œì»¬ ìºì‹œ, Driveì—” ì²´í¬í¬ì¸íŠ¸ë§Œ
- âœ… BF16 + TF32, í° ë°°ì¹˜, ì½œë ˆë²¨ í‰ê°€ë§Œ ë§¤ epoch
"""

# -*- coding: utf-8 -*-
"""KcELECTRA_MTL_Improved.ipynb

ğŸš€ ê°œì„ ëœ KcELECTRA MTL - A100 ìµœì í™” ë²„ì „

ì£¼ìš” ê°œì„ ì‚¬í•­:
- âœ… í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°•í™” (Sentiment 0: 2.3% â†’ ê°€ì¤‘ì¹˜ ì¦ê°€)
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
- âœ… Early Stopping + Best Model ë³µì›
- âœ… í´ë˜ìŠ¤ë³„ F1 ìƒì„¸ ì¶œë ¥
- âœ… ìºì‹œ ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
- âœ… í‰ê°€ ì†ë„ ê°œì„ 
"""

import torch, platform, warnings
warnings.filterwarnings('ignore')
print("PyTorch:", torch.__version__, "| CUDA:", torch.version.cuda)
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
print("Python:", platform.python_version())

# âš™ï¸ Config
import os, random, numpy as np, pandas as pd
from pathlib import Path

SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ê²½ë¡œ ì„¤ì •
DATA_DIR  = Path("/content/drive/MyDrive/project1/Processed_Data/MTL_ready_v3")  # âœ… v3 ì‚¬ìš©
SAVE_DIR  = Path("/content/drive/MyDrive/project1/kcelectra_mtl_ckpt_v3")
CACHE_DIR = Path("/content/kcelectra_mtl_cache")
for d in [SAVE_DIR, CACHE_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# AMP ì„¤ì •
bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()
amp_dtype = torch.bfloat16 if bf16_ok else torch.float16
if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True
    try:
        torch.set_float32_matmul_precision("medium")
    except:
        pass

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
BATCH_SIZE = 160
GRAD_ACCUM = 1
NUM_WORKERS = 4  # MODIFIED: 8â†’4ë¡œ ì¤„ì—¬ ë©”ëª¨ë¦¬ ì•ˆì •ì„± ë†’ì„ (A100ì—ì„œ ë°ë“œ ë°©ì§€)
PREFETCH = 4
PIN_MEMORY = True
PERSISTENT = True

MAX_LEN = 384
STRIDE = 192

EPOCHS = 15
LR = 3e-5
WARMUP_RATIO = 0.1
WEIGHT_DECAY = 0.01
MAX_GRAD_NORM = 1.0
LABEL_SMOOTHING = 0.1
DROPOUT_RATE = 0.3

# âœ… Task ê°€ì¤‘ì¹˜ ì¡°ì • (Sentimentê°€ ë” ì–´ë ¤ì›€)
ALPHA = 0.4  # Urgency (ê· í˜•ì¡í˜)
BETA = 0.6   # Sentiment (ë¶ˆê· í˜•)

# âœ… í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ gamma (MODIFIED: Urgency ë‚®ì¶¤ (ê· í˜•ì ), Sentiment ì¦ê°€)
GAMMA_URGENCY = 0.3  # ê· í˜• ë°ì´í„°ë¼ ë‚®ì¶¤
GAMMA_SENT = 0.8     # í´ë˜ìŠ¤ 0 ê°•í™”

MODEL_NAME = "beomi/KcELECTRA-base-v2022"
print(f"\nğŸ“‹ Config:")
print(f"  AMP dtype: {amp_dtype}")
print(f"  MAX_LEN/STRIDE: {MAX_LEN}/{STRIDE}")
print(f"  BATCH: {BATCH_SIZE} | GRAD_ACCUM: {GRAD_ACCUM}")
print(f"  Workers: {NUM_WORKERS} | Prefetch: {PREFETCH}")
print(f"  Task weights: Î±={ALPHA}, Î²={BETA}")
print(f"  Class gamma: Urgency={GAMMA_URGENCY}, Sentiment={GAMMA_SENT}")

# ğŸ“¥ Load MTL v3 data
def read_any(base: Path, stem: str):
    """Parquet â†’ CSV â†’ Feather ìˆœìœ¼ë¡œ ì‹œë„"""
    for ext, fn in [(".parquet", pd.read_parquet),
                    (".csv", pd.read_csv),
                    (".feather", pd.read_feather)]:
        fp = base / f"{stem}{ext}"
        if fp.exists():
            print(f"  â†’ {fp.name}")
            try:
                return fn(fp)
            except Exception as e:
                print(f"âš ï¸ Failed to read {fp.name}: {e}. Try installing engines or checking versions.")
    raise FileNotFoundError(f"{stem} not found in {base}")

print("\nğŸ“‚ Loading data...")
X_train = read_any(DATA_DIR, "X_train")
X_test  = read_any(DATA_DIR, "X_test")
y_u_tr  = read_any(DATA_DIR, "y_train_urgency")
y_u_te  = read_any(DATA_DIR, "y_test_urgency")
y_s_tr  = read_any(DATA_DIR, "y_train_sentiment")
y_s_te  = read_any(DATA_DIR, "y_test_sentiment")

def squeeze_label(obj, key=None):
    if isinstance(obj, pd.DataFrame):
        if key and key in obj.columns:
            return obj[key].astype(int)
        if obj.shape[1] == 1:
            return obj.iloc[:, 0].astype(int)
    return pd.Series(obj).astype(int)

y_u_tr = squeeze_label(y_u_tr, "y_urgency")
y_u_te = squeeze_label(y_u_te, "y_urgency")
y_s_tr = squeeze_label(y_s_tr, "y_sentiment")
y_s_te = squeeze_label(y_s_te, "y_sentiment")

text_tr = X_train["full_text"].astype(str)
text_te = X_test["full_text"].astype(str)

print(f"âœ… Data loaded: Train={len(text_tr):,}, Test={len(text_te):,}")

# âœ… í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
print("\nğŸ“Š Class Distribution:")
print("Urgency (Train):", y_u_tr.value_counts(normalize=True).sort_index().to_dict())
print("Sentiment (Train):", y_s_tr.value_counts(normalize=True).sort_index().to_dict())

# ğŸ”¤ Tokenizer & Model
from transformers import AutoTokenizer, AutoModel
import torch.nn as nn

print(f"\nğŸ”¤ Loading tokenizer: {MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

class ElectraMTL(nn.Module):
    def __init__(self, base_name: str, n_u: int = 3, n_s: int = 4, dropout=0.3):
        super().__init__()
        self.base = AutoModel.from_pretrained(base_name)
        hdim = self.base.config.hidden_size
        self.dropout = nn.Dropout(dropout)
        self.head_u = nn.Linear(hdim, n_u)
        self.head_s = nn.Linear(hdim, n_s)

    def forward(self, input_ids, attention_mask):
        out = self.base(input_ids=input_ids, attention_mask=attention_mask)
        cls = out.last_hidden_state[:, 0]
        z = self.dropout(cls)
        return self.head_u(z), self.head_s(z)

print("ğŸ—ï¸ Building model...")
model = ElectraMTL(MODEL_NAME, n_u=3, n_s=4, dropout=DROPOUT_RATE).to(device)
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"  Total params: {total_params:,}")
print(f"  Trainable params: {trainable_params:,}")

# ğŸ§± Tokenize + Chunk Cache
from tqdm.auto import tqdm

def build_chunk_cache_MTL(texts, y_u, y_s, tokenizer, split_name: str,
                          max_len=512, stride=256, batch_size=1024):
    """ì²­í¬ ë‹¨ìœ„ë¡œ í† í¬ë‚˜ì´ì§• + ìºì‹±"""
    cache_path = CACHE_DIR / f"{split_name}_ml{max_len}_st{stride}.pt"

    # âœ… ìºì‹œ ì¡´ì¬ ì‹œ ë¡œë”© (MODIFIED: dtype/shape ë¬´ê²°ì„± ì²´í¬ ì¶”ê°€)
    if cache_path.exists():
        try:
            print(f"âš¡ Loading cached chunks: {cache_path.name}")
            cached = torch.load(cache_path, weights_only=False)
            expected_keys = {"input_ids", "attention_mask", "y_u", "y_s", "sample_idx"}
            if expected_keys.issubset(cached.keys()):
                # ì¶”ê°€ ì²´í¬: dtypeê³¼ shape
                if cached["input_ids"].dtype == torch.int32 and cached["attention_mask"].dtype == torch.int8:
                    print(f"  Chunks: {cached['y_u'].size(0):,}")
                    return cached
                else:
                    print("  âš ï¸ Cache dtype/shape invalid, regenerating...")
            else:
                print("  âš ï¸ Invalid cache, regenerating...")
        except Exception as e:
            print(f"  âš ï¸ Cache load failed: {e}, regenerating...")

    # ìºì‹œ ìƒì„±
    print(f"ğŸ”¨ Building cache: {split_name} (MAX_LEN={max_len}, STRIDE={stride})")
    all_ids, all_masks, all_u, all_s, all_map = [], [], [], [], []
    N = len(texts)

    for i in tqdm(range(0, N, batch_size), desc=f"Tokenize [{split_name}]"):
        batch_texts = [str(t) for t in texts[i:i+batch_size]]

        enc = tokenizer(
            batch_texts,
            max_length=max_len,
            truncation=True,
            padding="max_length",
            return_attention_mask=True,
            return_overflowing_tokens=True,
            stride=stride,
            return_tensors="pt"
        )

        mapping = enc.pop("overflow_to_sample_mapping")
        global_map = (mapping + i).to(torch.int32)

        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ íƒ€ì… ì‚¬ìš©
        ids = enc["input_ids"].to(torch.int32)
        mask = enc["attention_mask"].to(torch.int8)

        labs_u = torch.tensor([int(y_u.iloc[int(j)]) for j in global_map],
                              dtype=torch.int64)
        labs_s = torch.tensor([int(y_s.iloc[int(j)]) for j in global_map],
                              dtype=torch.int64)

        all_ids.append(ids)
        all_masks.append(mask)
        all_u.append(labs_u)
        all_s.append(labs_s)
        all_map.append(global_map)

    tensors = {
        "input_ids": torch.cat(all_ids, 0),
        "attention_mask": torch.cat(all_masks, 0),
        "y_u": torch.cat(all_u, 0),
        "y_s": torch.cat(all_s, 0),
        "sample_idx": torch.cat(all_map, 0),
    }

    torch.save(tensors, cache_path)
    print(f"ğŸ’¾ Saved cache â†’ {cache_path.name} | Chunks: {tensors['y_u'].size(0):,}")
    return tensors

# ìºì‹œ ìƒì„±/ë¡œë”©
print("\nğŸ”„ Preparing chunks...")
tr_cache = build_chunk_cache_MTL(text_tr, y_u_tr, y_s_tr, tokenizer, "train", MAX_LEN, STRIDE)
va_cache = build_chunk_cache_MTL(text_te, y_u_te, y_s_te, tokenizer, "val", MAX_LEN, STRIDE)

# Dataset & DataLoader
class MTLChunkDataset(torch.utils.data.Dataset):
    def __init__(self, tensors):
        self.t = tensors

    def __len__(self):
        return self.t["y_u"].size(0)

    def __getitem__(self, i):
        return {
            "input_ids": self.t["input_ids"][i].long(),
            "attention_mask": self.t["attention_mask"][i].bool(),
            "y_u": self.t["y_u"][i],
            "y_s": self.t["y_s"][i],
        }

from torch.utils.data import DataLoader

def make_loader(ds, batch_size, shuffle):
    try:
        return DataLoader(
            ds, batch_size=batch_size, shuffle=shuffle,
            num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,
            prefetch_factor=PREFETCH, persistent_workers=PERSISTENT
        )
    except Exception as e:
        print(f"âš ï¸ DataLoader fallback (workers=0): {e}")
        return DataLoader(ds, batch_size=batch_size, shuffle=shuffle,
                         num_workers=0, pin_memory=False)

print("\nğŸšš Creating DataLoaders...")
train_loader = make_loader(MTLChunkDataset(tr_cache), BATCH_SIZE, True)
val_loader = make_loader(MTLChunkDataset(va_cache), BATCH_SIZE * 2, False)  # MODIFIED: í•„ìš” ì‹œ BATCH_SIZE * 1.5ë¡œ ì¡°ì • ê°€ëŠ¥
print(f"  Train batches: {len(train_loader):,}")
print(f"  Val batches: {len(val_loader):,}")

# ğŸ”§ Optimizer / Loss / Scheduler
from torch.optim import AdamW
from transformers import get_cosine_schedule_with_warmup
from torch.optim.lr_scheduler import ReduceLROnPlateau  # MODIFIED: ì¶”ê°€ (plateau ì‹œ LR ê°ì†Œ)

def make_class_weights(y_np, gamma=0.5, device='cpu'):
    """í´ë˜ìŠ¤ ë¹ˆë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (gammaë¡œ ì¡°ì •)"""
    cnt = torch.bincount(torch.tensor(y_np, dtype=torch.long))
    w = (cnt.sum() / (len(cnt) * cnt.float().clamp(min=1))).pow(gamma)
    return w.to(device)

print("\nâš–ï¸ Computing class weights...")
wu = make_class_weights(y_u_tr.values, GAMMA_URGENCY, device=device)
ws = make_class_weights(y_s_tr.values, GAMMA_SENT, device=device)
print(f"  Urgency weights: {wu.tolist()}")
print(f"  Sentiment weights: {ws.tolist()}")

loss_u = nn.CrossEntropyLoss(weight=wu, label_smoothing=LABEL_SMOOTHING).to(device)
loss_s = nn.CrossEntropyLoss(weight=ws, label_smoothing=LABEL_SMOOTHING).to(device)

# Optimizer (weight decay ì œì™¸ íŒŒë¼ë¯¸í„°)
no_decay = ["bias", "LayerNorm.weight"]
optimizer_grouped_parameters = [
    {
        "params": [p for n, p in model.named_parameters()
                   if not any(nd in n for nd in no_decay)],
        "weight_decay": WEIGHT_DECAY
    },
    {
        "params": [p for n, p in model.named_parameters()
                   if any(nd in n for nd in no_decay)],
        "weight_decay": 0.0
    },
]
optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)

# Scheduler
total_steps = max(1, EPOCHS * len(train_loader) // GRAD_ACCUM)
warmup_steps = int(total_steps * WARMUP_RATIO)
scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)
reduce_lr = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)
print(f"\nğŸ“… Scheduler: {total_steps:,} total steps, {warmup_steps:,} warmup steps")

# GradScaler
from torch.cuda.amp import GradScaler
scaler = GradScaler(enabled=torch.cuda.is_available())

# ğŸ“ Evaluation Functions
from sklearn.metrics import f1_score, classification_report  # MODIFIED: classification_report ì¶”ê°€ (PR/Recall ì¶œë ¥)
from torch.amp import autocast

@torch.no_grad()
def evaluate_call_level(model, loader, cache_tensors):
    """Call-level í‰ê°€ (ì²­í¬ ë¡œì§“ í‰ê· )"""
    model.eval()
    logits_u_list, logits_s_list = [], []
    labels_u_list, labels_s_list = [], []

    for batch in tqdm(loader, desc="Eval", leave=False):
        ids = batch["input_ids"].to(device, non_blocking=True)
        mask = batch["attention_mask"].to(device, non_blocking=True)
        yu = batch["y_u"].to(device, non_blocking=True)
        ys = batch["y_s"].to(device, non_blocking=True)

        with autocast('cuda', dtype=amp_dtype, enabled=torch.cuda.is_available()):
            lu, ls = model(ids, mask)

        # MODIFIED: FP32 ìºìŠ¤íŒ… ê°•í™”
        logits_u_list.append(lu.detach().float().cpu().numpy())
        logits_s_list.append(ls.detach().float().cpu().numpy())
        labels_u_list.append(yu.detach().cpu().numpy())
        labels_s_list.append(ys.detach().cpu().numpy())

    # ì—°ê²°
    all_logits_u = np.concatenate(logits_u_list, 0)
    all_logits_s = np.concatenate(logits_s_list, 0)
    all_labels_u = np.concatenate(labels_u_list, 0)
    all_labels_s = np.concatenate(labels_s_list, 0)

    idx = cache_tensors["sample_idx"].cpu().numpy()

    # Call-level ì§‘ê³„
    results = {}
    for task_name, logits, ytrue in [
        ("urgency", all_logits_u, all_labels_u),
        ("sentiment", all_logits_s, all_labels_s)
    ]:
        preds, gts = [], []
        for sid in np.unique(idx):
            mask = (idx == sid)
            pooled_logits = logits[mask].mean(0)
            preds.append(pooled_logits.argmax())
            gts.append(ytrue[mask][0])

        preds = np.array(preds)
        gts = np.array(gts)

        results[task_name] = {
            "micro": f1_score(gts, preds, average="micro"),
            "macro": f1_score(gts, preds, average="macro"),
            "weighted": f1_score(gts, preds, average="weighted"),
            "per_class": f1_score(gts, preds, average=None).tolist(),
            "predictions": preds,
            "ground_truth": gts,
            "report": classification_report(gts, preds, output_dict=True)  # MODIFIED: PR/Recall ì¶”ê°€
        }

    return results

# ğŸƒ Training Loop
print("\nğŸƒ Starting training...")
best_combo = -1.0
best_epoch = 0
patience = 3  # MODIFIED: 5â†’3ìœ¼ë¡œ ì¤„ì—¬ ì˜¤ë²„í”¼íŒ… ë°©ì§€
patience_counter = 0
history = []

for epoch in range(1, EPOCHS + 1):
    model.train()
    running_loss = 0.0
    optimizer.zero_grad(set_to_none=True)

    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS}", leave=True)  # MODIFIED: leave=Trueë¡œ ì¶œë ¥ ê°œì„ 
    for step, batch in enumerate(pbar):
        ids = batch["input_ids"].to(device, non_blocking=True)
        mask = batch["attention_mask"].to(device, non_blocking=True)
        tu = batch["y_u"].to(device, non_blocking=True)
        ts = batch["y_s"].to(device, non_blocking=True)

        with autocast('cuda', dtype=amp_dtype, enabled=torch.cuda.is_available()):
            lu, ls = model(ids, mask)
            loss = (ALPHA * loss_u(lu, tu) + BETA * loss_s(ls, ts)) / GRAD_ACCUM

        scaler.scale(loss).backward()

        if (step + 1) % GRAD_ACCUM == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
            optimizer.zero_grad(set_to_none=True)

        running_loss += loss.item() * GRAD_ACCUM
        pbar.set_postfix({
            "loss": f"{loss.item() * GRAD_ACCUM:.4f}",
            "lr": f"{scheduler.get_last_lr()[0]:.2e}"
        })

    avg_loss = running_loss / len(train_loader)

    # Call-level í‰ê°€
    val_results = evaluate_call_level(model, val_loader, va_cache)

    urgW = val_results['urgency']['weighted']
    sentW = val_results['sentiment']['weighted']
    combo = 0.5 * urgW + 0.5 * sentW

    # MODIFIED: í´ë˜ìŠ¤ë³„ ìƒì„¸ ë³´ê³ ì„œ ì¶œë ¥ (PR/Recall í¬í•¨)
    print(f"\n{'='*70}")
    print(f"Epoch {epoch}/{EPOCHS} | Loss: {avg_loss:.4f}")
    print(f"{'='*70}")
    print(f"Urgency  â†’ Weighted F1: {urgW:.4f} | Per-class F1: {val_results['urgency']['per_class']}")
    print(f"  Report: {val_results['urgency']['report']}")
    print(f"Sentiment â†’ Weighted F1: {sentW:.4f} | Per-class F1: {val_results['sentiment']['per_class']}")
    print(f"  Report: {val_results['sentiment']['report']}")
    print(f"Combo (0.5U + 0.5S): {combo:.4f}")

    # íˆìŠ¤í† ë¦¬ ì €ì¥
    history.append({
        "epoch": epoch,
        "loss": avg_loss,
        "urg_weighted": urgW,
        "sent_weighted": sentW,
        "combo": combo,
        "urg_per_class": val_results['urgency']['per_class'],
        "sent_per_class": val_results['sentiment']['per_class']
    })

    # Best model ì €ì¥
    if combo > best_combo:
        best_combo = combo
        best_epoch = epoch
        patience_counter = 0

        torch.save({
            "state_dict": model.state_dict(),
            "history": history,
            "best_combo": best_combo,
            "epoch": epoch,
            "config": {
                "alpha": ALPHA, "beta": BETA,
                "max_len": MAX_LEN, "stride": STRIDE,
                "gamma_u": GAMMA_URGENCY, "gamma_s": GAMMA_SENT,
                "dropout": DROPOUT_RATE,
                "lr": LR, "batch_size": BATCH_SIZE
            }
        }, SAVE_DIR / "best_mtl.pt")

        print(f"âœ… Best model saved! (combo={best_combo:.4f})")
    else:
        patience_counter += 1
        print(f"â³ No improvement ({patience_counter}/{patience})")

        if patience_counter >= patience:
            print(f"\nâ›” Early stopping at epoch {epoch}")
            break

    # MODIFIED: Reduce LR on plateau (combo ê¸°ë°˜)
    reduce_lr.step(combo)

# âœ… Best model ë³µì›
print(f"\n{'='*70}")
print("ğŸ† Training completed!")
print(f"{'='*70}")
ckpt_path = SAVE_DIR / "best_mtl.pt"
if ckpt_path.exists():
    state = torch.load(ckpt_path, map_location=device, weights_only=False)
    model.load_state_dict(state["state_dict"])
    model.to(device)
    print(f"âœ… Loaded best model from epoch {state['epoch']}")
    print(f"   Best combo: {state['best_combo']:.4f}")

    # Final evaluation
    print("\nğŸ“Š Final Evaluation on Test Set:")
    final_results = evaluate_call_level(model, val_loader, va_cache)

    print(f"\nUrgency:")
    print(f"  Weighted F1: {final_results['urgency']['weighted']:.4f}")
    print(f"  Per-class F1: {final_results['urgency']['per_class']}")
    print(f"  Report: {final_results['urgency']['report']}")

    print(f"\nSentiment:")
    print(f"  Weighted F1: {final_results['sentiment']['weighted']:.4f}")
    print(f"  Per-class F1: {final_results['sentiment']['per_class']}")
    print(f"  Report: {final_results['sentiment']['report']}")

    # âœ… í˜¼ë™ í–‰ë ¬ ì¶œë ¥
    from sklearn.metrics import confusion_matrix

    print("\nğŸ“ˆ Confusion Matrix (Urgency):")
    cm_u = confusion_matrix(final_results['urgency']['ground_truth'],
                            final_results['urgency']['predictions'])
    print(cm_u)

    print("\nğŸ“ˆ Confusion Matrix (Sentiment):")
    cm_s = confusion_matrix(final_results['sentiment']['ground_truth'],
                            final_results['sentiment']['predictions'])
    print(cm_s)

# ğŸ¯ Inference Function
@torch.no_grad()
def predict_text(texts, aggregate="mean"):
    """ê¸´ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ (ì²­í¬ í‰ê· )"""
    if isinstance(texts, str):
        texts = [texts]

    model.eval()
    results = []

    for text in texts:
        enc = tokenizer(
            text,
            max_length=MAX_LEN,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
            return_overflowing_tokens=True,
            stride=STRIDE
        ).to(device)

        with autocast('cuda', dtype=amp_dtype, enabled=torch.cuda.is_available()):
            lu, ls = model(
                enc["input_ids"],
                enc["attention_mask"]
            )
            prob_u = lu.softmax(-1).float()  # MODIFIED: FP32 ìºìŠ¤íŒ…
            prob_s = ls.softmax(-1).float()  # MODIFIED: FP32 ìºìŠ¤íŒ…

        # ì²­í¬ í‰ê· 
        if aggregate == "mean":
            avg_u = prob_u.mean(0)
            avg_s = prob_s.mean(0)
        else:  # max
            avg_u = prob_u.max(0).values
            avg_s = prob_s.max(0).values

        pred_u = int(avg_u.argmax().item())
        pred_s = int(avg_s.argmax().item())
        conf_u = float(avg_u.max().item())
        conf_s = float(avg_s.max().item())

        # MODIFIED: Confidence threshold ì˜ˆì‹œ (0.5 ë¯¸ë§Œ ì‹œ ê²½ê³ )
        if conf_u < 0.5 or conf_s < 0.5:
            print("âš ï¸ Low confidence prediction")

        results.append({
            "urgency": pred_u,
            "sentiment": pred_s,
            "urgency_conf": conf_u,
            "sentiment_conf": conf_s
        })

    return results[0] if len(results) == 1 else results

# í…ŒìŠ¤íŠ¸
print("\nğŸ§ª Test Inference:")
test_text = "[TURN=12] [TURN_BIN=M] [IRESP=UNK] [IRESP_BIN=UNK] [CALLER] ì•ˆë…•í•˜ì„¸ìš” ê¸‰í•œ ì¼ì´ ìˆì–´ì„œìš” [AGENT] ë„¤ ë§ì”€í•˜ì„¸ìš”"
result = predict_text(test_text)
print(f"Input: {test_text[:100]}...")
print(f"Prediction: {result}")

print("\nâœ… All done!")